{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent using NumPy vs PyTorch"
      ],
      "metadata": {
        "id": "6NeNdOnzN0Yg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the 5th tutorial from Patrick Loeber's PyTorch tutorial playlist."
      ],
      "metadata": {
        "id": "pfmOkAVmxBJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "7qZSAdVJOE-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a linear regression model that has to predict the model as y = 2x."
      ],
      "metadata": {
        "id": "lIWxv5yPPsVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss=MSE\n",
        "def loss(y, y_predicted):\n",
        "  return((y_predicted - y)**2).mean()\n",
        "\n",
        "# Function --> 1/N (w*x - y_pred)**2\n",
        "# Gradient --> 1/N 2*(w*x - y_pred)*x\n",
        "def gradient(x, y, y_predicted):\n",
        "  return np.dot(2*x, y_predicted-y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 10  # number of iterations\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradient\n",
        "  dw = gradient(X, Y, y_pred)\n",
        "\n",
        "  # update weights acc to gradient descent.\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1mnji-jOGa6",
        "outputId": "01033016-adcc-4d64-e3d1-546b415166b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 2: w = 1.680, loss = 4.79999924\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 4: w = 1.949, loss = 0.12288000\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "epoch 6: w = 1.992, loss = 0.00314574\n",
            "epoch 7: w = 1.997, loss = 0.00050331\n",
            "epoch 8: w = 1.999, loss = 0.00008053\n",
            "epoch 9: w = 1.999, loss = 0.00001288\n",
            "epoch 10: w = 2.000, loss = 0.00000206\n",
            "Prediction after training: f(5) = 9.999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, creating the same model but the gradient descent is calculated using PyTorch."
      ],
      "metadata": {
        "id": "0Y7PMEtoSshj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor([0.], dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss=MSE\n",
        "def loss(y, y_predicted):\n",
        "  return((y_predicted - y)**2).mean()\n",
        "\n",
        "# This manual gradient function isn't needed now.\n",
        "\n",
        "# the Tensor.item() function is always applied on a tensor that only has 1 value. This function returns the value of tensor as a scalar.\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')  \n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100  # number of iterations\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradient\n",
        "  # dw = gradient(X, Y, y_pred)\n",
        "  l.backward()\n",
        "\n",
        "  # update weights acc to gradient descent.\n",
        "  # Since the updation of weights should not be a part of gradient tracking, we have to use no_grad function here.\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  # Set the gradients back to zero to prevent accumulation\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if epoch%10 == 0:\n",
        "    print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOiWEFLIR9c2",
        "outputId": "135b5086-1b07-49b1-f237-6505a291e9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 61: w = 2.000, loss = 0.00000010\n",
            "epoch 71: w = 2.000, loss = 0.00000000\n",
            "epoch 81: w = 2.000, loss = 0.00000000\n",
            "epoch 91: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we're gonna do everything using PyTorch. So, we create the same model using PyTorch.\n",
        "\n",
        "# Training Pipeline in PyTorch\n",
        "\n",
        "We have 3 steps in this - \n",
        "1. Design our model (input size and output size, all the layers)\n",
        "2. Construct the loss and the optimizer.\n",
        "3. Training loop\n",
        "- forward pass: Compute the gradient\n",
        "- backward pass: Gradients\n",
        "- update weights\n",
        "- set gradients to zero\n",
        "- repeat."
      ],
      "metadata": {
        "id": "NtbVo8ZBTzu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "# w = torch.tensor([0.], dtype=torch.float32, requires_grad=True) Dont need this now. Model already knows.\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# model prediction\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# the Tensor.item() function is always applied on a tensor that only has 1 value. This function returns the value of tensor as a scalar.\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.06\n",
        "n_iters = 250  # number of iterations\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Stochastic Gradient Descent\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradient\n",
        "  # dw = gradient(X, Y, y_pred)\n",
        "  l.backward()\n",
        "\n",
        "  # update weights acc to gradient descent.\n",
        "  optimizer.step()\n",
        "\n",
        "  # Set the gradients back to zero to prevent accumulation\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch%10 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l.item():.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
      ],
      "metadata": {
        "id": "DU-SpwLpJ2HR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95fd056-87fc-4b57-be52-9e25358ec7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = -2.282\n",
            "epoch 1: w = 1.836, loss = 46.52779388\n",
            "epoch 11: w = 1.859, loss = 0.02960750\n",
            "epoch 21: w = 1.883, loss = 0.02060417\n",
            "epoch 31: w = 1.902, loss = 0.01433865\n",
            "epoch 41: w = 1.918, loss = 0.00997842\n",
            "epoch 51: w = 1.932, loss = 0.00694408\n",
            "epoch 61: w = 1.943, loss = 0.00483245\n",
            "epoch 71: w = 1.953, loss = 0.00336295\n",
            "epoch 81: w = 1.960, loss = 0.00234031\n",
            "epoch 91: w = 1.967, loss = 0.00162865\n",
            "epoch 101: w = 1.972, loss = 0.00113340\n",
            "epoch 111: w = 1.977, loss = 0.00078874\n",
            "epoch 121: w = 1.981, loss = 0.00054890\n",
            "epoch 131: w = 1.984, loss = 0.00038198\n",
            "epoch 141: w = 1.987, loss = 0.00026582\n",
            "epoch 151: w = 1.989, loss = 0.00018499\n",
            "epoch 161: w = 1.991, loss = 0.00012874\n",
            "epoch 171: w = 1.992, loss = 0.00008959\n",
            "epoch 181: w = 1.994, loss = 0.00006235\n",
            "epoch 191: w = 1.995, loss = 0.00004339\n",
            "epoch 201: w = 1.996, loss = 0.00003019\n",
            "epoch 211: w = 1.996, loss = 0.00002101\n",
            "epoch 221: w = 1.997, loss = 0.00001462\n",
            "epoch 231: w = 1.997, loss = 0.00001018\n",
            "epoch 241: w = 1.998, loss = 0.00000708\n",
            "Prediction after training: f(5) = 9.996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing a custom Linear Regression Model in PyTorch."
      ],
      "metadata": {
        "id": "TsjNoz3psJsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "# w = torch.tensor([0.], dtype=torch.float32, requires_grad=True) Dont need this now. Model already knows.\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# model prediction\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    # Initialize the super class.\n",
        "    super(LinearRegression, self).__init__()\n",
        "    \n",
        "    # Define layers.\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "\n",
        "# the Tensor.item() function is always applied on a tensor that only has 1 value. This function returns the value of tensor as a scalar.\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.06\n",
        "n_iters = 250  # number of iterations\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Stochastic Gradient Descent\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradient\n",
        "  # dw = gradient(X, Y, y_pred)\n",
        "  l.backward()\n",
        "\n",
        "  # update weights acc to gradient descent.\n",
        "  optimizer.step()\n",
        "\n",
        "  # Set the gradients back to zero to prevent accumulation\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch%10 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l.item():.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baCOpTIDu4GI",
        "outputId": "b7de42bd-0e44-4df1-de99-e41682b7e49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 2.555\n",
            "epoch 1: w = 1.729, loss = 15.53926945\n",
            "epoch 11: w = 1.772, loss = 0.07775649\n",
            "epoch 21: w = 1.810, loss = 0.05411150\n",
            "epoch 31: w = 1.841, loss = 0.03765675\n",
            "epoch 41: w = 1.868, loss = 0.02620573\n",
            "epoch 51: w = 1.890, loss = 0.01823686\n",
            "epoch 61: w = 1.908, loss = 0.01269120\n",
            "epoch 71: w = 1.923, loss = 0.00883195\n",
            "epoch 81: w = 1.936, loss = 0.00614624\n",
            "epoch 91: w = 1.947, loss = 0.00427724\n",
            "epoch 101: w = 1.955, loss = 0.00297656\n",
            "epoch 111: w = 1.963, loss = 0.00207142\n",
            "epoch 121: w = 1.969, loss = 0.00144153\n",
            "epoch 131: w = 1.974, loss = 0.00100317\n",
            "epoch 141: w = 1.978, loss = 0.00069812\n",
            "epoch 151: w = 1.982, loss = 0.00048582\n",
            "epoch 161: w = 1.985, loss = 0.00033809\n",
            "epoch 171: w = 1.987, loss = 0.00023528\n",
            "epoch 181: w = 1.990, loss = 0.00016374\n",
            "epoch 191: w = 1.991, loss = 0.00011394\n",
            "epoch 201: w = 1.993, loss = 0.00007930\n",
            "epoch 211: w = 1.994, loss = 0.00005518\n",
            "epoch 221: w = 1.995, loss = 0.00003840\n",
            "epoch 231: w = 1.996, loss = 0.00002672\n",
            "epoch 241: w = 1.996, loss = 0.00001860\n",
            "Prediction after training: f(5) = 9.994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O42hlR6sveMB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}